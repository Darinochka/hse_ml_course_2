{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4cbb9d",
   "metadata": {},
   "source": [
    "Прежде чем сдать эту работу, убедитесь, что всё выполняется так, как ожидается.\n",
    "Сначала перезапустите ядро (в меню выберите Kernel → Restart), а затем запустите все ячейки (в меню выберите Cell → Run All).\n",
    "\n",
    "Убедитесь, что вы заполнили все места, где указано YOUR CODE HERE или \"YOUR ANSWER HERE\", а также указали своё имя ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21511039",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcde98",
   "metadata": {},
   "source": [
    "Убедитесь, что используете версию Python > 3. Все тетрадки отрабатывают точно на версии 3.10.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276dccb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12f230",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "af12f230",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84f09368a8b772d3e9b8681cc8686b0e",
     "grade": false,
     "grade_id": "cell-dd7054e4036af1fd",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Домашнее задание 2. Градиентный спуск своими руками\n",
    "\n",
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных модификаций градиентного спуска. **В ячейках ниже в ноутбуке** вам нужно будет реализовать несколько классов для различных вариаций градиентного спуска, а именно:\n",
    "* `VanillaGradientDescent`\n",
    "* `StochasticGradientDescent`\n",
    "* `MomentumDescent`\n",
    "* `Adam`\n",
    "\n",
    "Также вам необходимо будет реализовать класс `LinearRegression` для обучения линейной регрессии (и, разумеется, предсказания целевой переменной на основе обученной модели)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09006ad9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "09006ad9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ffdce5d2f333a7b1edd235192d04bce",
     "grade": false,
     "grade_id": "cell-5e184e91596808f6",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 1. Реализация градиентного спуска\n",
    "\n",
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на шаблоны в ячейках ниже.\n",
    "\n",
    "**Все реализуемые методы должны быть векторизованы!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb06bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5dbb06bd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a86f573904df6b6ae093bbc96c02c17c",
     "grade": false,
     "grade_id": "cell-0e43c241c775eeff",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Напоминание про градиентный спуск\n",
    "\n",
    "Основное свойство антиградиента &ndash; он указывает в сторону *наискорейшего* убывания функции в данной точке. Соответственно, будет логично стартовать из некоторой точки, сдвинуться в сторону антиградиента,\n",
    "пересчитать антиградиент и снова сдвинуться в его сторону и т.д. Запишем это более формально.\n",
    "\n",
    "Пусть $w_0$ &ndash; начальный набор параметров (например, нулевой или сгенерированный из некоторого\n",
    "случайного распределения). Тогда ванильный градиентный спуск состоит в повторении следующих шагов до сходимости:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "Здесь $\\eta_{k}$ обозначает длину шага на $k$-ой итерации (learning rate), а $Q(w)$ - функцию потерь (loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e80d2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "89e80d2b",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ee23bf7a83b28a2b8ebc3d4286b8981",
     "grade": false,
     "grade_id": "cell-0323321d1eb23427",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 1.1. Learning Rate Schedules (0.06 балла)\n",
    "\n",
    "**Learning rate** (скорость обучения) $\\eta$ — это гиперпараметр, который определяет величину шага при обновлении весов модели в направлении антиградиента. Если learning rate слишком большой, градиентный спуск может «перепрыгивать» через минимум и расходиться; если слишком маленький — обучение будет идти крайне медленно. Поэтому часто используют **расписания** (schedules), которые адаптивно меняют $\\eta$ в процессе обучения: например, начинают с большого шага для быстрого продвижения и постепенно уменьшают его для более точной сходимости.\n",
    "\n",
    "С помощью  класса `LearningRateSchedule` мы на каждой итерации градиентного спуска будем получать соответствующий `learning_rate` $\\eta_k$.\n",
    "\n",
    "В ячейке ниже уже реализован класс `ConstantLR`, который на каждой итерации возвращает один и тот же заранее заданный шаг. Ваша задача в этом пункте – реализовать `TimeDecayLR`, который мы будем использовать для обучения линейной регрессии. Формула очередного шага должна выглядеть следующим образом:\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f0d71",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1371f9959fc1e815a1e5bfe98e6e73b4",
     "grade": false,
     "grade_id": "cell-45726bba7e13000a",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "class LearningRateSchedule(ABC):\n",
    "    @abstractmethod\n",
    "    def get_lr(self, iteration: int) -> float:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ConstantLR(LearningRateSchedule):\n",
    "    def __init__(self, lr: float):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_lr(self, iteration: int) -> float:\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "class TimeDecayLR(LearningRateSchedule):\n",
    "    def __init__(self, lambda_: float = 0.1):\n",
    "        self.s0 = 1\n",
    "        self.p = 0.5\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def get_lr(self, iteration: int) -> float:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc535ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9b2d1d263da7b06e720a2837003c940",
     "grade": false,
     "grade_id": "cell-7fb08a3a7d37f6f6",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Выполните ячейку ниже для проверки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4293667",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5feff48988d8ce2011b3b20ba324eb20",
     "grade": true,
     "grade_id": "cell-b9289cc9f6892f9a",
     "llm_graded": false,
     "locked": true,
     "points": 6,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr = TimeDecayLR(lambda_=1.0)\n",
    "assert abs(lr.get_lr(0) - 1.0) < 1e-9, \"get_lr(0) должен быть 1.0\"\n",
    "assert lr.get_lr(1) < lr.get_lr(0), \"Шаг должен убывать с итерацией\"\n",
    "assert abs(lr.get_lr(3) - (1.0 * (1/(1+3))**0.5)) < 1e-9\n",
    "\n",
    "assert abs(lr.get_lr(99) - (1.0 * (1/100)**0.5)) < 1e-9\n",
    "lr2 = TimeDecayLR(lambda_=0.5)\n",
    "assert abs(lr2.get_lr(0) - 0.5) < 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc313e5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bc313e5e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7234f0d27cee1ce09435b85d00111729",
     "grade": false,
     "grade_id": "cell-3f9eb2b82ddf09bd",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание-примечание 1.1. Родительский класс BaseDescent (0 баллов).\n",
    "\n",
    "Ниже приведен шаблон класса `BaseDescent` – родительского класса для модификаций градиентного спуска, от которого будут наследоваться другие классы (`VanillaGradientDescent`, `StochasticGradientDescent`, `MomentumDescent` и `Adam`). Более подробно про наследование классов в Python можно прочитать\n",
    "* Наследование: https://docs.python.org/3/tutorial/classes.html#inheritance\n",
    "* Абстрактные классы: https://docs.python.org/3/library/abc.html\n",
    "\n",
    "В классе `BaseDescent` **все методы уже реализованы**. Цель этого задания – внимательно ознакомиться с тем, как устроен этот класс.\n",
    "\n",
    "Обратите внимание на атрибут `self.iteration`, отвечающий за номер итерации алгоритма спуска. Как раз с помощью него (и `self.lr_schedule`) мы и будем получать `learning_rate` на соответствующей итерации алгоритма. Функция `update_weights` должна обновлять веса модели `self.model.w`, а также возвращать величину обновления $w_{k + 1} - w_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU9cU-ax_uFR",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VU9cU-ax_uFR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08db00704218008b2253e76020ec5a18",
     "grade": false,
     "grade_id": "cell-266fd5d36db633f8",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Обратите внимание**\n",
    "\n",
    "Все реализуемые вами классы спуска в задании - это *универсальные* оптимизаторы. Они не должны считать градиенты конкретной функции потерь внутри себя.\n",
    "\n",
    "Для вычисления градиента они всегда обращаются к модели, с которой работают:\n",
    "\n",
    "```\n",
    "gradient = self.model.compute_gradients(X_batch, y_batch)\n",
    "```\n",
    "\n",
    "Чтобы это работало, уже на данном этапе должны быть реализованы в классе `LinearRegression`:\n",
    "\n",
    "* `compute_gradients(X, y)` для MSE (в дальнейшем, в Задании 7, сюда добавляется член L2-регуляризации),\n",
    "* `compute_loss(X, y)` для MSE (аналогично с учётом L2 при необходимости).\n",
    "\n",
    "Если идёте строго по порядку, реализуйте эти MSE-версии в начале Задания 2.1, а затем вернитесь к заданиям 1.2–1.6 — код оптимизаторов менять не придётся.\n",
    "\n",
    "Такой подход позволяет свободно менять функцию потерь, не переписывая алгоритмы оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ea063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d78e08fe22ab2b3275f8353ed9448990",
     "grade": false,
     "grade_id": "cell-6b43b80a94a8065b",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BaseDescent(ABC):\n",
    "    def __init__(self, lr_schedule: LearningRateSchedule = TimeDecayLR()):\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.iteration = 0\n",
    "        self.model = None\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_weights(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        update = self.update_weights()\n",
    "        self.iteration += 1\n",
    "        return update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b83e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3f9b83e1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36a676824e2f1025ab900a093584301e",
     "grade": false,
     "grade_id": "cell-65adf8e7aa621cf2",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 1.2. Полный градиентный спуск VanillaGradientDescent (0.5 балла).\n",
    "\n",
    "Реализуйте полный градиентный спуск, заполнив пропуски в классе `VanillaGradientDescent` в ячейке ниже. Напомним, что шаг классического градиентного спуска выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Важно**: Здесь и далее функция `update_weights` должна возвращать разницу между \n",
    "\n",
    "$w_{k + 1}$ и $w_{k}$: $\\quad w_{k + 1} - w_{k} = -\\eta_{k} \\nabla_{w} Q(w_{k})$\n",
    "\n",
    "Кроме того, соответственно своему названию, она должна обновлять веса модели `model.w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dd5e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "762e21cdf2c80175ea52ce738201b536",
     "grade": false,
     "grade_id": "cell-347d330bde48c6eb",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class VanillaGradientDescent(BaseDescent):\n",
    "    def update_weights(self):\n",
    "        X_train = self.model.X_train\n",
    "        y_train = self.model.y_train\n",
    "        gradient = self.model.compute_gradients(X_train, y_train)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63d3f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1f63d3f4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5be23620ef6a8bbe73e74f0c1a5a309a",
     "grade": false,
     "grade_id": "cell-aa62006d572a7eeb",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Напоминание про SGD (стохастических градиентный спуск)\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w)$ представим в виде суммы $\\ell$ функций:\n",
    "\n",
    "$$\n",
    "    Q(w)\n",
    "    =\n",
    "    \\frac{1}{\\ell}\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        q_i(w).\n",
    "$$\n",
    "\n",
    "В нашем домашнем задании отдельные функции $q_i(w)$ соответствуют ошибкам на отдельных объектах.\n",
    "\n",
    "Проблема метода градиентного спуска состоит в том, что на каждом шаге необходимо вычислять градиент всей суммы (будем его называть полным градиентом):\n",
    "\n",
    "$$\n",
    "    \\nabla_w Q(w)\n",
    "    =\n",
    "    \\frac{1}{\\ell}\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\nabla_w q_i(w).\n",
    "$$\n",
    "\n",
    "Это может быть очень трудоёмко при больших размерах выборки. В то же время точное вычисление градиента может быть не так уж необходимо &ndash; как правило, мы делаем не очень большие шаги в сторону антиградиента, и наличие в нём неточностей не должно сильно сказаться на общей траектории.\n",
    "\n",
    "Оценить градиент суммы функций можно средним градиентов случайно взятого подмножества функций:\n",
    "\n",
    "$$\n",
    "    \\nabla_{w} Q(w_{k}) \\approx \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}),\n",
    "$$\n",
    "где $B$ - это случайно выбранное подмножество индексов, обычно называемое **батчом**.\n",
    "\n",
    "Оценка $\\frac{1}{|B|} \\sum \\limits_{i \\in B} \\nabla_w q_i(w_k)$ называется **стохастическим градиентом** функции потерь, а получившийся метод называют методом **стохастического градиентного спуска** или просто SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528caa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7528caa0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1d15a561babf59d76a6f25f72d7468",
     "grade": false,
     "grade_id": "cell-0a722d08764f3c75",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 1.3. Стохастический градиентный спуск StochasticGradientDescent (0.7 баллов).\n",
    "\n",
    "Реализуйте стохастический градиентный спуск, заполнив пропуски в классе `StochasticGradientDescent`. Шаг оптимизации:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}).\n",
    "$$\n",
    "\n",
    "Размер батча будет являться **гиперпараметром** метода и передаваться в конструктор класса `__init__(...)`. Семплировать индексы батча объектов $B$ можно с повторениями (через np.random.randint) - это допустимо и даёт несмещённую оценку градиента. По желанию можно без повторений (np.random.choice(..., replace=False) или через пермутацию по эпохам)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bad96",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f33b19e6fbf9510c10cd172354c38fd",
     "grade": false,
     "grade_id": "cell-e2368c9f45a4bea6",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "class StochasticGradientDescent(BaseDescent):\n",
    "    def __init__(self, lr_schedule: LearningRateSchedule = TimeDecayLR(), batch_size=1):\n",
    "        super().__init__(lr_schedule)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_weights(self):\n",
    "        X_train = self.model.X_train\n",
    "        y_train = self.model.y_train\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d8c32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7c5d8c32",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "511b4cf4114c33d29295b241ae245aeb",
     "grade": false,
     "grade_id": "cell-ea324e51c2b96846",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Напоминание про метод инерции (или метод моментов)\n",
    "\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Например, если линии уровня функционала сильно вытянуты, то из-за ортогональности градиента линиям уровня он будет менять направление на почти противоположное на каждом шаге. Такие осцилляции будут вносить сильный шум в движение, и процесс оптимизации займёт много итераций. Чтобы избежать этого, можно усреднять векторы антиградиента с нескольких предыдущих шагов &ndash; в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введём для этого вектор инерции:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k})\n",
    "\\end{align}\n",
    "\n",
    "Здесь $\\alpha$ &ndash; параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Разумеется, вместо вектора градиента может быть использована его аппроксимация (например, в случае **стохастического градиентного спуска**). Чтобы сделать шаг градиентного спуска, просто сдвинем предыдущую точку на вектор инерции:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "Заметим, что если по какой-то координате градиент постоянно меняет знак, то в результате усреднения градиентов в векторе инерции эта координата окажется близкой к нулю. Если же по координате знак градиента всегда одинаковый, то величина соответствующей координаты в векторе инерции будет большой, и мы будем делать большие шаги в соответствующем направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53187c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f53187c3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39fb3a10e6970f5b85aedd067ac646fb",
     "grade": false,
     "grade_id": "cell-728967f23fdbdcac",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 1.5 Метод Momentum - MomentumDescent (1 балл).\n",
    "\n",
    "Реализуйте градиентный спуск с методом инерции заполнив пропуски в классе `MomentumDescent`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\beta h_{k} + \\eta_k \\nabla_w Q(w_{k}) \\\\\n",
    "    &w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\beta$ является гиперпараметром метода, однако в данном домашнем задании мы зафиксируем её за вас $\\beta = 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50fd3c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4c65f7f431d04892fa0591f88662050",
     "grade": false,
     "grade_id": "cell-547c4d6e3d9c29a4",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    def __init__(self, lr_schedule: LearningRateSchedule = TimeDecayLR(), beta=0.9):\n",
    "        super().__init__(lr_schedule)\n",
    "        self.beta = beta\n",
    "        self.velocity = None\n",
    "\n",
    "    def update_weights(self):\n",
    "        X_train = self.model.X_train\n",
    "        y_train = self.model.y_train\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602977f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "602977f8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3f196690ec1cc2524dd4e3bd575899a",
     "grade": false,
     "grade_id": "cell-83f96089fdb007b3",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Напоминание про AdaGrad, RMSprop и Adam\n",
    "\n",
    "Градиентный спуск очень чувствителен к выбору длины шага. Если шаг большой, то есть риск, что мы будем перескакивать через точку минимума; если же шаг маленький, то для нахождения минимума потребуется много итераций. При этом нет способов заранее определить правильный размер шага &ndash; к тому же, схемы с постепенным уменьшением шага по мере итераций могут тоже плохо работать.\n",
    "\n",
    "В методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров. Идея проста: мы будем \"копить\" сумму квадратов градиентов и делить очередной градиент на корень из этой суммы. Таким образом, обновление весов с большими градиентами будет тормозиться, а с маленькими наоборот получать большие шаги. Формула обновлени будет выглядить так:\n",
    "\n",
    "\\begin{align}\n",
    "    &G_{kj} = G_{k-1,j} + (\\nabla_w Q(w_{k - 1}))_j^2; \\\\\n",
    "    &w_{jk} = w_{j,k-1} - \\frac{\\eta_t}{\\sqrt{G_{kj}} + \\varepsilon} (\\nabla_w Q(w_{k - 1}))_j.\n",
    "\\end{align}\n",
    "\n",
    "Здесь $\\varepsilon$ небольшая константа, которая предотвращает деление на ноль.\n",
    "\n",
    "В данном методе можно зафиксировать длину шага (например, $\\eta_k = 0.01$) и не подбирать её в процессе обучения **(обратите внимание, что в данном домашнем задании длина шага не фиксируется)**. Отметим, что данный метод подходит для разреженных задач, в которых у каждого объекта большинство признаков равны нулю. Для признаков, у которых ненулевые значения встречаются редко, будут делаться большие шаги; если же какой-то признак часто является ненулевым, то шаги по нему будут небольшими.\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт, из-за чего шаги становятся всё медленнее и могут остановиться ещё до того, как достигнут минимум функционала. Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$$\n",
    "    G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2.\n",
    "$$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "Можно объединить идеи описанных выше методов: накапливать градиенты со всех прошлых шагов для\n",
    "избежания осцилляций (метод инерции), а также делать адаптивную длину шага по каждому параметру (`RMSProp`). Таким образом, мы получим метод `Adam` с той лишь разницей, что в методе `Adam` дополнительно делается нормировка накопленных градиентов и квадратов градиентов для устранения смещения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010de93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1010de93",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac4a555dff2be2216758d734f7ee5a8",
     "grade": false,
     "grade_id": "cell-d32e8db51d7e9b49",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 1.6. Метод Adam (Adaptive Moment Estimation) (1.2 балла).\n",
    "\n",
    "Реализуйте градиентный спуск с методом Adam, заполнив пропуски в классе `Adam`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &m_0 = 0, \\quad v_0 = 0; \\\\ \\\\\n",
    "    &m_{k + 1} = \\beta_1 m_k + (1 - \\beta_1) \\nabla_w Q(w_{k}); \\\\ \\\\\n",
    "    &v_{k + 1} = \\beta_2 v_k + (1 - \\beta_2) \\left(\\nabla_w Q(w_{k})\\right)^2; \\\\ \\\\\n",
    "    &\\widehat{m}_{k} = \\dfrac{m_k}{1 - \\beta_1^{k}}, \\quad \\widehat{v}_{k} = \\dfrac{v_k}{1 - \\beta_2^{k}}; \\\\ \\\\\n",
    "    &w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\widehat{v}_{k + 1}} + \\varepsilon} \\widehat{m}_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\beta_1 = 0.9, \\beta_2 = 0.999$ и $\\varepsilon = 10^{-8}$ будут зафиксированы за вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d3afe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffb7b4e6c6bafe10b7845cfdd61d7a21",
     "grade": false,
     "grade_id": "cell-f37e66dcab631dcb",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Adam(BaseDescent):\n",
    "    def __init__(self, lr_schedule: LearningRateSchedule = TimeDecayLR(), beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(lr_schedule)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update_weights(self):\n",
    "        X_train = self.model.X_train\n",
    "        y_train = self.model.y_train\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f85f2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c8f85f2a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0b3a30998db896851fcee6cb9b7c5b2",
     "grade": false,
     "grade_id": "cell-d0ee7e51ab615185",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 2. Линейная регресия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a383b3",
   "metadata": {},
   "source": [
    "Напомним, что функция потерь MSE записывается как:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\frac{1}{n} \\sum \\limits_{i = 1}^n (y_i - \\langle x_i, w \\rangle)^2 = \\frac{1}{n} \\| X w - y \\|^2\n",
    "$$\n",
    "\n",
    "где $n$ – количество объектов в выборке, $X \\in \\mathbb{R}^{n \\times d}$ – матрица \"объект-признак\", а $y \\in \\mathbb{R}^n$ – целевая переменная. Через $x_i$ обозначается $i$-ая строчка матрицы $X$, отвечающая за $i$-й объект выборки.\n",
    "\n",
    "Выпишите ниже (подсмотрев в семинар или решив самостоятельно) градиент для функции потерь MSE в матричном виде."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60efc21",
   "metadata": {
    "deletable": false,
    "id": "b60efc21",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86b96c0d06bb3fe10ba1810ccff82885",
     "grade": true,
     "grade_id": "cell-8b1bf0fc5de78216",
     "llm_graded": true,
     "locked": false,
     "points": 4,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Задание 2.1. Градиент MSE в матричном виде (0.04 балла).\n",
    "Напомним, что функция потерь MSE записывается как:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\frac{1}{n} \\sum \\limits_{i = 1}^n (y_i - \\langle x_i, w \\rangle)^2 = \\frac{1}{n} \\| X w - y \\|^2\n",
    "$$\n",
    "\n",
    "где $n$ – количество объектов в выборке, $X \\in \\mathbb{R}^{n \\times d}$ – матрица \"объект-признак\", а $y \\in \\mathbb{R}^n$ – целевая переменная. Через $x_i$ обозначается $i$-ая строчка матрицы $X$, отвечающая за $i$-й объект выборки.\n",
    "\n",
    "Выпишите ниже (подсмотрев в семинар или решив самостоятельно) градиент для функции потерь MSE в матричном виде.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e76c4",
   "metadata": {},
   "source": [
    "### Задание 2.2. Решение методом градиентного спуска (1 балл)\n",
    "\n",
    "В этом задании вам предстоит написать свою реализацию линейной регрессии, обучаемой с использованием градиентного спуска, с опорой на шаблон в ячейках выше — класс **LinearRegression**. По сути линейная регрессия будет оберткой, которая запускает обучение\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`;\n",
    "    * Разность весов содержит наны;\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Будем считать, что все данные, которые поступают на вход имеют столбец единичек последним столбцом;\n",
    "* Используйте `optimizer.step()` для обновления весов\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту) и значение функции потерь после оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82564d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import auto, Enum\n",
    "from typing import Optional\n",
    "\n",
    "class LossFunction(Enum):\n",
    "    MSE = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58cddf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae725abfe670a51b5ea5f2a405b36eef",
     "grade": false,
     "grade_id": "cell-7dfa7803d453d46d",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optional[BaseDescent | str] = None,\n",
    "        l2_coef: float = 0.0,\n",
    "        tolerance: float = 1e-6,\n",
    "        max_iter: int = 1000,\n",
    "        loss_function: LossFunction = LossFunction.MSE\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        if isinstance(optimizer, BaseDescent):\n",
    "            self.optimizer.set_model(self)\n",
    "        self.l2_coef = l2_coef\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_function = loss_function\n",
    "        self.w = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Подсказка: предсказание линейной модели — это матричное умножение датасета на веса\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_gradients(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # Выше вы выводили формулу градиента для MSE\n",
    "        # Не забудьте про слагаемое регуляризации: + l2_coef * w (для 7 задания)\n",
    "        if self.loss_function is LossFunction.MSE:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    def compute_loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        if self.loss_function is LossFunction.MSE:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X_train, self.y_train = X, y\n",
    "        n, d = X.shape\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Аналитическое решение (optimizer is None)\n",
    "        # можете пока игнорировать, это следующее задание\n",
    "        if self.optimizer is None:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            self.loss_history.append(self.compute_loss(X, y))\n",
    "            return\n",
    "\n",
    "        # Градиентный спуск\n",
    "        #   1. Инициализируйте веса нулями\n",
    "        #   2. Запишите начальный loss в loss_history\n",
    "        #   3. В цикле до max_iter:\n",
    "        #      - обновите веса с помощью функции `optimizer.step()`\n",
    "        #      - проверьте критерии останова\n",
    "        #      - если не остановились — запишите текущий loss в loss_history\n",
    "        if isinstance(self.optimizer, BaseDescent):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5_DqmlOkXX_",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I5_DqmlOkXX_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bae8c703703307a7077b64e2d6b4ea6",
     "grade": false,
     "grade_id": "cell-97946411f028e2dd",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 2.3. Аналитическое решение (0.5 балла)\n",
    "\n",
    "Но, как мы помним из лекции, помимо решения при помощи градиентного спуска, для ряда функций потерь можно выписать в том числе аналитическое решение. Давайте сперва вспомним, как оно выглядит для MSE. Выведите оптимальную формулу для $w$, держа в памяти формулу MSE, и дополните класс `LinearRegression`\n",
    "\n",
    "$$\\text{MSE} = \\| X w - y \\|^2$$\n",
    "$$ w = $$\n",
    "\n",
    "**Вопрос**: Как мы помним, у аналитического решения есть минусы, какие кстати?\n",
    "\n",
    "**Ответ**: ответ не оценивается. Вспомните, что мы обсуждали на лекции\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75174d72",
   "metadata": {},
   "source": [
    "Теперь добавьте это решение в блок `LinearRegression` для случая, когда `optimizedr=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110855d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "110855d6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aadf0c158250eecbe9e1cee4806f7cde",
     "grade": false,
     "grade_id": "cell-fe07f7d168a48104",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "x = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FVQkfptaFQ4P",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FVQkfptaFQ4P",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a409baf904ff2902699091bd28d10914",
     "grade": true,
     "grade_id": "cell-3f4330020a55a39e",
     "llm_graded": false,
     "locked": true,
     "points": 50,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sklearn\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "sklearn_linreg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "sklearn_linreg.fit(x, y)\n",
    "\n",
    "your_linreg = LinearRegression(optimizer=None)\n",
    "your_linreg.fit(x, y)\n",
    "\n",
    "assert abs(mse(your_linreg.predict(x), y) - mse(sklearn_linreg.predict(x), y)) < 1e-12, \"Не повезло, попробуйте еще раз\"\n",
    "\n",
    "x2 = np.random.rand(50, 4); y2 = np.random.rand(50)\n",
    "lr2 = LinearRegression(optimizer=None); lr2.fit(x2, y2)\n",
    "assert lr2.w.shape == (4,)\n",
    "sk2 = sklearn.linear_model.LinearRegression(fit_intercept=False).fit(x2, y2)\n",
    "assert abs(mse(lr2.predict(x2), y2) - mse(sk2.predict(x2), y2)) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZNtnKIk9LVsT",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZNtnKIk9LVsT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94c603f7d664bcb53f4b91112c633d43",
     "grade": false,
     "grade_id": "cell-71f9b36d2afb1997",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WD28hZzlLNAB",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WD28hZzlLNAB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50bbf5e4f6dfb235b00b593a8e9f549d",
     "grade": false,
     "grade_id": "cell-1a6f35dedc1d338f",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Данная секция нужна для того, чтобы убедиться в правильности реализации методов спуска и класса `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d0c02-f973-4622-9218-fdbb3b6673a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b8de36de055d24a2354b115ab1da18",
     "grade": false,
     "grade_id": "cell-a4ee017e3d0fbf83",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0\n",
    "num_objects = 100\n",
    "dimension = 10\n",
    "x = np.random.rand(num_objects, dimension)\n",
    "y = x @ np.random.rand(dimension) + 2 + np.random.random(num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "USsgi0BdLElg",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "USsgi0BdLElg",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0193447a815af963da739337ae90430",
     "grade": true,
     "grade_id": "cell-366957dde987e52f",
     "llm_graded": false,
     "locked": true,
     "points": 50,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = VanillaGradientDescent()\n",
    "model = LinearRegression(optimizer=optimizer, max_iter=max_iter, tolerance=tolerance)\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "assert y_pred.shape == y.shape, \"VanillaGradientDescent: Prediction shape does not match\"\n",
    "assert len(model.loss_history) == max_iter + 1, \"Loss history failed\"\n",
    "assert model.loss_history[-1] < model.loss_history[0], \"Loss history failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360898d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4ff90eac4d63f9c3280b04aa01d98fb",
     "grade": true,
     "grade_id": "cell-b1ea8906bc939326",
     "llm_graded": false,
     "locked": true,
     "points": 70,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = StochasticGradientDescent(batch_size=5)\n",
    "model = LinearRegression(optimizer=optimizer, max_iter=max_iter, tolerance=tolerance)\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "assert y_pred.shape == y.shape, \"StochasticGradientDescent: Prediction shape does not match\"\n",
    "assert len(model.loss_history) == max_iter + 1, \"Loss history failed\"\n",
    "assert model.loss_history[-1] < model.loss_history[0], \"Loss history failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47964b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2109ea59e915fe9c2d8400363ebdb40",
     "grade": true,
     "grade_id": "cell-85bf9b0cc55cb7a2",
     "llm_graded": false,
     "locked": true,
     "points": 100,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = MomentumDescent()\n",
    "model = LinearRegression(optimizer=optimizer, max_iter=max_iter, tolerance=tolerance)\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "assert y_pred.shape == y.shape, \"MomentumDescent: Prediction shape does not match\"\n",
    "assert len(model.loss_history) == max_iter + 1, \"Loss history failed\"\n",
    "assert model.loss_history[-1] < model.loss_history[0], \"Loss history failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133648e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d54c00011318a9bfb771175bf46270ca",
     "grade": true,
     "grade_id": "cell-14849f88ab4fda68",
     "llm_graded": false,
     "locked": true,
     "points": 120,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "model = LinearRegression(optimizer=optimizer, max_iter=max_iter, tolerance=tolerance)\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "assert y_pred.shape == y.shape, \"Adam: Prediction shape does not match\"\n",
    "assert len(model.loss_history) == max_iter + 1, \"Loss history failed\"\n",
    "assert model.loss_history[-1] < model.loss_history[0], \"Loss history failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86807ebc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaac7ee59bc34a8ce3e53e7220f2bb42",
     "grade": true,
     "grade_id": "cell-49eed65255ae2272",
     "llm_graded": false,
     "locked": true,
     "points": 100,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as SkLinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sk_model = SkLinearRegression(fit_intercept=False)\n",
    "sk_model.fit(x, y)\n",
    "y_sk = sk_model.predict(x)\n",
    "\n",
    "my_model = LinearRegression(optimizer=None)\n",
    "my_model.fit(x, y)\n",
    "y_my = my_model.predict(x)\n",
    "\n",
    "assert y_sk.shape == y_my.shape == y.shape, \"LinearRegression: Prediction shape does not match target\"\n",
    "assert abs(mean_squared_error(y_sk, y) - mean_squared_error(y_my, y)) < 1e-10, \"LinearRegression: MSEs differ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586dcbc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5586dcbc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9cc56bbbf53f0798dbbdbd91c1d85d",
     "grade": false,
     "grade_id": "cell-d50cf8c63847b8f6",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 4. Работа с данными\n",
    "\n",
    "Представьте, что вы — инженер-эколог в крупной аналитической компании, которая помогает автопроизводителям и регуляторам понимать реальные выбросы углекислого газа.\n",
    "\n",
    "Вам передали большой набор данных об автомобилях разных лет, классов, типов топлива и двигателей. Ваша задача — построить модель, которая по техническим характеристикам машины с максимально возможной точностью предскажет, сколько граммов CO₂ будет выбрасывать этот автомобиль на каждую милю пути.\n",
    "\n",
    "В этом задании вам необходимо проанализировать данные для будущего обучения. Напрямую за EDA оценка не ставится, но от него зависит будущая метрика. За метрику оценка ставится."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a35652",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90e70fcce6061454ad224451e494ea4c",
     "grade": false,
     "grade_id": "cell-efed332cfa264780",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Целевой признак (то, что предсказываем):\n",
    "\n",
    "- **co2TailpipeGpm** — выбросы CO₂ из выхлопной трубы, г/милю\n",
    "\n",
    "Доступные признаки:\n",
    "\n",
    "- displ — рабочий объём двигателя, литры\n",
    "- cylinders — количество цилиндров\n",
    "- year — год выпуска модели\n",
    "- comb — комбинированный расход топлива (город + трасса), миль на галлон\n",
    "- highway — расход на трассе, MPG\n",
    "- city — расход в городе, MPG\n",
    "- fuelCost — расчётная годовая стоимость топлива, $\n",
    "- fuelType1 — тип основного топлива \n",
    "- drive — тип привода\n",
    "- trany — тип трансмиссии \n",
    "- VClass — класс автомобиля по классификации EPA \n",
    "- tCharger — наличие турбонаддува (T = есть)\n",
    "- sCharger — наличие механического нагнетателя (S = есть)\n",
    "- guzzler — автомобиль попадает под «gas guzzler tax»\n",
    "- startStop — наличие системы start-stop (Y = есть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bb408",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b28bb408",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aef9a08c25120387de4688ca438f99f4",
     "grade": false,
     "grade_id": "cell-66b3b35d42432182",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Классы ConstantLR, TimeDecayLR, VanillaGradientDescent, StochasticGradientDescent,\n",
    "# MomentumDescent, Adam, SAGDescent и LinearRegression определены выше в ноутбуке\n",
    "\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df11d8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d70c7eb957fa0d75f3ad65b175c2e37",
     "grade": false,
     "grade_id": "cell-eb0213d171c6fce0",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "У вас уже есть разделенные выборки на train и test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be065ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2be065ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c86a35f6a07020ceb9321ad3d096a292",
     "grade": false,
     "grade_id": "cell-255deb060d59d776",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('train.csv')\n",
    "x_test = pd.read_csv('test_with_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055f2b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6055f2b2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff539873c6ad24f95dfde425e221c11c",
     "grade": false,
     "grade_id": "cell-1e635a443726e527",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    },
    "outputId": "2ccdd548-beaa-4068-fdca-49ed12016c16"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751ce80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c751ce80",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c43138554a56af4f65f05f62a00a5efd",
     "grade": false,
     "grade_id": "cell-7800b3df9a1f39b3",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Добавляем в данные единичную колонку `bias`, чтобы не делать отдельные параметр $b$ для свободного члена модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8351dbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a8351dbf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9934a9b617ea413c41c59a50915b6f09",
     "grade": false,
     "grade_id": "cell-766fa41f3fab989d",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train['bias'] = 1\n",
    "x_test['bias'] = 1\n",
    "\n",
    "y_train = x_train['co2TailpipeGpm']\n",
    "x_train = x_train.drop(columns=['co2TailpipeGpm'])\n",
    "\n",
    "y_test = x_test['co2TailpipeGpm']\n",
    "x_test = x_test.drop(columns=['co2TailpipeGpm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da5993",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8dfb2e28442fa14f5d0075abb37c532",
     "grade": false,
     "grade_id": "cell-fc6667e4e67865ca",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Теперь вам необходимо сделать EDA для последующего обучения:\n",
    "\n",
    "* Постройте распределение таргетовой метрики. Посмотрите, необходимо ли его логарфмировать? \n",
    "* Постройте графики между фичами и таргетовой метрикой. Здесь вам очень поможет `sns.pairplot`. Есть ли какая-то очевидная зависимость между таргетом и какой-то из фич? Если да, эта зависимость линейная? Есть ли линейно зависимые признаки?\n",
    "* Постройте распределение категориальных фич. Можно ли какие-то из них упростить? Содержательны ли они? Можно ли из них создать новые фичи? \n",
    "* Посмотрите, есть ли пропуски в данных. Подумайте, что с ними делать как для численных признаков, так и для категориальных\n",
    "\n",
    "* Если вы добавляете\\удаляете\\меняете фичу на x_train, не забывайте то же самое делать и для x_test.\n",
    "\n",
    "* В следующем задании вам необходимо будет подобрать гиперпараметры, поэтому разделите выборку x_train на x_train и x_valid c test_size=0.2. На x_valid вы будете подбирать гиперпараметры. Не забудьте по честное деление без даталиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af594fc",
   "metadata": {
    "deletable": false,
    "id": "3af594fc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4af8a582c8c22bd453c768195d8efa91",
     "grade": false,
     "grade_id": "cell-bf601d6af5080eee",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# используйте названия для датасета x_train, x_val, y_train, y_val\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833334b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fe377bc2d1ff38829a10e5be4af01fd",
     "grade": false,
     "grade_id": "cell-cb48bdd924b57c2c",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "После обработки фич разделите их на категориальные, числовые и другие (например, bias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca322b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad394217cbf280462ff2069f267df752",
     "grade": false,
     "grade_id": "cell-9356e0032d52f9ce",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# categorical = []\n",
    "# numeric = []\n",
    "# other = [\"bias\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bda096",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38235d93114a3bcc442665a7e72c3828",
     "grade": false,
     "grade_id": "cell-4fab12c28bf1291f",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train[categorical + numeric + other]\n",
    "x_val = x_val[categorical + numeric + other]\n",
    "x_test = x_test[categorical + numeric + other]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3ae6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63a7b276a84d7be88999417cbad86024",
     "grade": false,
     "grade_id": "cell-4967e7aa19bf688f",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Теперь используем `Pipeline`:\n",
    "- с помощью него можно закодировать категориальные фичи с помощью `OneHotEncoder`\n",
    "- нормализовать численные фичи с помощью `StandardScaler` или `MinMaxScaler`\n",
    "- также если вы не заполнили наны, полезен бывает `SimpleImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f618ad",
   "metadata": {
    "deletable": false,
    "id": "c4f618ad",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdfed0e20b8ff22aa7cd5590aa405211",
     "grade": false,
     "grade_id": "cell-a845691a503ecfd4",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    # при желании можете добавить SimpleImputer\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('numeric', numeric_pipeline, numeric),\n",
    "    ('other', 'passthrough', other)\n",
    "])\n",
    "\n",
    "# .toarray() необходим, если вы используете категориальные фичи\n",
    "# в ином случае можете это удалить\n",
    "x_train = column_transformer.fit_transform(x_train).toarray()\n",
    "x_val = column_transformer.transform(x_val).toarray()\n",
    "x_test = column_transformer.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a343f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "901a343f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bf23acff2e51652c54c216be6c00a8b",
     "grade": false,
     "grade_id": "cell-c06a2d6f686ec0d1",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)\n",
    "\n",
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877ce0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2e877ce0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cbece6d962ac314056c2691ce66e962",
     "grade": false,
     "grade_id": "cell-4e5186a50e55cee9",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 5.1. Подбор оптимальной длины шага (1 балл)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9757b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea5f5710d716ac8d2147e83f3894a27",
     "grade": false,
     "grade_id": "cell-f54df9e4c94498aa",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Реализуйте функцию, котоаря принимает на вход `x_train, y_train, x_valid, y_valid, optimizer`, обучает `LinearRegression` и возвращает лосс на валидационном сете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141c823",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c9850ab0df6b2b9e3778d1c3fec1323",
     "grade": false,
     "grade_id": "cell-ba2b78d7458e83e8",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e7e6d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcaa4f28f861670439c9832bcbd795ae",
     "grade": false,
     "grade_id": "cell-8a9760d20af68c39",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_valid, y_valid, optimizer: BaseDescent | None):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a92a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac5f51d42d57012a7ccb33fe762bbc1a",
     "grade": true,
     "grade_id": "cell-7eca12b7ae823819",
     "llm_graded": false,
     "locked": true,
     "points": 0,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_n, _d = 200, 5\n",
    "np.random.seed(42)\n",
    "_X_tm = np.random.randn(_n, _d)\n",
    "_y_tm = _X_tm @ np.random.randn(_d) + np.random.randn(_n) * 0.1\n",
    "_X_tm_val = np.random.randn(50, _d)\n",
    "_y_tm_val = _X_tm_val @ np.random.randn(_d)\n",
    "\n",
    "_opt = VanillaGradientDescent(lr_schedule=TimeDecayLR(lambda_=0.01))\n",
    "_loss = train_model(_X_tm, _y_tm, _X_tm_val, _y_tm_val, _opt)\n",
    "assert isinstance(_loss, (float, np.floating)), \"train_model должна возвращать float\"\n",
    "assert not np.isnan(_loss), \"Loss не должен быть NaN\"\n",
    "assert _loss > 0, \"Loss должен быть положительным\"\n",
    "\n",
    "_loss_analytic = train_model(_X_tm, _y_tm, _X_tm_val, _y_tm_val, None)\n",
    "assert isinstance(_loss_analytic, (float, np.floating)), \"train_model должна возвращать float для optimizer=None\"\n",
    "assert not np.isnan(_loss_analytic), \"Loss (аналитическое решение) не должен быть NaN\"\n",
    "\n",
    "_opt_sgd = StochasticGradientDescent(lr_schedule=TimeDecayLR(lambda_=0.01), batch_size=16)\n",
    "_loss_sgd = train_model(_X_tm, _y_tm, _X_tm_val, _y_tm_val, _opt_sgd)\n",
    "assert isinstance(_loss_sgd, (float, np.floating)), \"train_model должна возвращать float для SGD\"\n",
    "assert not np.isnan(_loss_sgd), \"Loss (SGD) не должен быть NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc88a9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4745451689b03826934e4497b0e649ef",
     "grade": false,
     "grade_id": "cell-b26d9d502094d768",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Для каждого из четырёх методов (`VanillaGradientDescent`, `StochasticGradientDescent`, `MomentumDescent`, `Adam`) подберите наилучшую начальную длину шага $\\lambda$ (`lambda_` в `TimeDecayLR`).\n",
    "\n",
    "**Алгоритм:**\n",
    "1. Задайте массив кандидатов `lambdas_` с помощью **логарифмической сетки** (`np.logspace`)\n",
    "2. Для каждого метода переберите все значения `lambda_`:\n",
    "   - создайте `TimeDecayLR(lambda_=lambda_)` и передайте в `optimizer`\n",
    "   - вызовите `train_model(x_train, y_train, x_val, y_val, optimizer)` — она вернёт loss на валидации;\n",
    "   - запомните пару `(lambda_, loss)`.\n",
    "3. Выберите `best_lambda` — ту лямбду, при которой loss на валидации минимален\n",
    "4. С лучшей лямбдой **заново обучите** модель на `x_train` и заполните словарь `report`:\n",
    "\n",
    "```python\n",
    "report[optimizer_name] = {\n",
    "    \"best_lambda\": best_lambda,         # лучшая лямбда\n",
    "    \"r2_test\": ...,                     # R² на тестовой выборке (x_test, y_test)\n",
    "    \"r2_train\": ...,                    # R² на обучающей выборке\n",
    "    \"loss_history\": model.loss_history, # история loss на обучении\n",
    "    \"loss_test\": ...,                   # loss на тестовой выборке\n",
    "    \"loss_train\": ...,                  # loss на обучающей выборке\n",
    "}\n",
    "```\n",
    "\n",
    "Все гиперпараметры кроме `lambda_` оставьте по умолчанию (batch_size, alpha, и т.д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b592f67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbddbaa7077215f1e0714f4158014567",
     "grade": false,
     "grade_id": "cell-7247a3c97ed78f44",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"VanillaGradientDescent\": VanillaGradientDescent, \n",
    "    \"StochasticGradientDescent\": StochasticGradientDescent, \n",
    "    \"MomentumDescent\": MomentumDescent, \n",
    "    \"Adam\": Adam\n",
    "}\n",
    "report = {\n",
    "    \"VanillaGradientDescent\": {}, \n",
    "    \"StochasticGradientDescent\": {}, \n",
    "    \"MomentumDescent\": {}, \n",
    "    \"Adam\": {}\n",
    "}\n",
    "\n",
    "report = {optimizer_name: {\n",
    "    \"loss_history\": None, \n",
    "    \"loss_test\": None, \n",
    "    \"loss_train\": None,\n",
    "    \"best_lambda\": None, \n",
    "    \"r2_test\": None,\n",
    "    \"r2_train\": None\n",
    "    } for optimizer_name in optimizers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1fc03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3be0aa0eb51e885264e7d2c30e33b02",
     "grade": false,
     "grade_id": "cell-c34244ea82a79768",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lambdas_ = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    lambda2loss = dict()\n",
    "    \n",
    "    for lambda_ in tqdm(lambdas_):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "    best_lambda = min(lambda2loss, key=lambda2loss.get)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c6012-5479-4a99-9fd6-666accb9d8da",
   "metadata": {},
   "source": [
    "За перебор 0,2 балла. За метрику R2 на тесте >= 0.85 - 0,4 балла, за метрику R2 на тесте >= 0.98 - 0,4 балла. Всего 1 балл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225e9d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb63e0d79b2f6e5bdc684560c24d16e5",
     "grade": true,
     "grade_id": "cell-932f53f3c805d2cd",
     "llm_graded": false,
     "locked": true,
     "points": 20,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_required_keys = {\"best_lambda\", \"r2_test\", \"r2_train\", \"loss_history\", \"loss_test\", \"loss_train\"}\n",
    "\n",
    "for opt_name in optimizers:\n",
    "    assert opt_name in report\n",
    "    entry = report[opt_name]\n",
    "    for key in _required_keys:\n",
    "        assert key in entry\n",
    "\n",
    "    assert entry[\"best_lambda\"] is not None\n",
    "    assert entry[\"best_lambda\"] > 0\n",
    "\n",
    "    assert entry[\"r2_test\"] is not None\n",
    "    assert entry[\"r2_test\"] > 0\n",
    "\n",
    "    assert entry[\"loss_history\"] is not None and len(entry[\"loss_history\"]) > 1\n",
    "    assert entry[\"loss_history\"][-1] < entry[\"loss_history\"][0]\n",
    "\n",
    "    assert entry[\"loss_test\"] is not None\n",
    "    assert not np.isnan(entry[\"loss_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841234f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cf1ff2138320bedf208daac55bfd053",
     "grade": true,
     "grade_id": "cell-5a3d4add0dc09154",
     "llm_graded": false,
     "locked": true,
     "points": 40,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert any(report[opt][\"r2_test\"] >= 0.85 for opt in optimizers), \"No optimizer has r2 >= 0.85\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73137fdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ced81c8a494c67dc0e25728e4e46191a",
     "grade": true,
     "grade_id": "cell-1032916162d7d93d",
     "llm_graded": false,
     "locked": true,
     "points": 40,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert any(report[opt][\"r2_test\"] >= 0.98 for opt in optimizers), \"No optimizer has r2 >= 0.98\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07913925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "07913925",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a6a7d3548b12b7a130c2be434968266",
     "grade": false,
     "grade_id": "cell-e3dc6d5b8c95dca0",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 5.2. Сравнение методов (1 балл)\n",
    "\n",
    "* Постройте график зависимости ошибки на обучающей выборке от номера итерации (все методы на одном графике).\n",
    "* Постройте сравнительную таблицу каждого метода с лучшими параметрами и метриками (loss, r2)\n",
    "* Балл будет выставлен именно за выводы. Если вы показали графики, но не сделали выводов - 0 баллов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61f205",
   "metadata": {
    "deletable": false,
    "id": "4c61f205",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e4302a05f4e6a3b4309c922274b72ef",
     "grade": false,
     "grade_id": "cell-9a002afcf6ef83e7",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed898c8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f04c349edb68d789c9bc49f8082b4f9",
     "grade": false,
     "grade_id": "cell-2383dd7c48b9e227",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizers = [\"VanillaGradientDescent\", \"StochasticGradientDescent\", \"MomentumDescent\", \"Adam\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18d066",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "Напишите **в следующей ячейке** свои выводы в этой ячейке (где и вопрос) о сравнении: \n",
    "- какой метод набрал наивысший R2?\n",
    "- какой метод сошелся быстрее остальных?\n",
    "- отличаются ли порядком лямбды для каждого из методов?\n",
    "- можете написать другие свои инсайты при анализе данных, что помогло повысить метрику? (дополнительно)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388eef4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca84866add033b21492f21e09517b1d",
     "grade": true,
     "grade_id": "cell-f380ccd3930d3719",
     "llm_graded": true,
     "locked": false,
     "points": 100,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d325d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "46d325d5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151aef5eb16a7324e5f4bfdc888641b8",
     "grade": false,
     "grade_id": "cell-556b3c88a2e81b2c",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 6. Стохастический градиентный спуск и размер батча (2 балла)\n",
    "\n",
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54979c59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afe04732d1efb42d8a4f4ffb44d87099",
     "grade": false,
     "grade_id": "cell-c3124e44310f8745",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 6.1. Функция для замера эксперимента (0.5 балла)\n",
    "\n",
    "Реализуйте функцию `run_sgd_experiment`, которая:\n",
    "1. Создаёт `SGD` с заданным `batch_size` и `lr_schedule` и модель линейной регрессии.\n",
    "2. Замеряет время обучения (используйте модуль `time`).\n",
    "3. Считает количество итераций до сходимости — это `len(model.loss_history)`\n",
    "4. Возвращает словарь `{\"time\": <float>, \"iterations\": <int>}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e8340",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd5d905cb9548e3b1dd11c7a0186e5d",
     "grade": false,
     "grade_id": "cell-84cb774e4c0cb608",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_sgd_experiment(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int,\n",
    "    max_iter: int = 5000,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Запускает SGD с заданным batch_size и возвращает словарь\n",
    "    {\"time\": <время в секундах>, \"iterations\": <число итераций до сходимости>}.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e397a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbfed6eeadfd7c229cf0c61d61ef0889",
     "grade": true,
     "grade_id": "cell-ec254a3b4fd3bcde",
     "llm_graded": false,
     "locked": true,
     "points": 50,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_test_n, _test_d = 200, 5\n",
    "np.random.seed(42)\n",
    "_X_test6 = np.random.randn(_test_n, _test_d)\n",
    "_y_test6 = _X_test6 @ np.random.randn(_test_d) + np.random.randn(_test_n) * 0.1\n",
    "\n",
    "result = run_sgd_experiment(_X_test6, _y_test6, batch_size=32)\n",
    "\n",
    "assert isinstance(result, dict), \"run_sgd_experiment должна возвращать dict\"\n",
    "assert \"time\" in result and \"iterations\" in result, \\\n",
    "    'Словарь должен содержать ключи \"time\" и \"iterations\"'\n",
    "assert isinstance(result[\"time\"], float), '\"time\" должен быть float (секунды)'\n",
    "assert isinstance(result[\"iterations\"], (int, np.integer)), '\"iterations\" должен быть int'\n",
    "assert result[\"time\"] > 0, \"Время должно быть положительным\"\n",
    "assert result[\"iterations\"] > 0, \"Число итераций должно быть положительным\"\n",
    "\n",
    "r1 = run_sgd_experiment(_X_test6, _y_test6, batch_size=1, max_iter=500)\n",
    "r_full = run_sgd_experiment(_X_test6, _y_test6, batch_size=_test_n, max_iter=500)\n",
    "assert r1[\"iterations\"] > 0 and r_full[\"iterations\"] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10efac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df2e84e72e0c093a3bdc8e131842c07b",
     "grade": false,
     "grade_id": "cell-5d307101079677e6",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 6.2. Сбор статистики по размерам батча (0.5 балла)\n",
    "\n",
    "Для каждого размера батча из списка `batch_sizes` сделайте $k$ запусков (`n_runs`) функции `run_sgd_experiment` и посчитайте **среднее** время и **среднее** число итераций.\n",
    "\n",
    "Сохраните результаты в словарь `batch_stats`, где ключ  размер батча, а значение словарь `{\"mean_time\": ..., \"mean_iterations\": ...}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c208c",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e957ae670d7a325efcd2f6c6a8ac9c82",
     "grade": false,
     "grade_id": "cell-17763f4fd30ece64v",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "batch_sizes = [1, 2, 4, 64, 128, 512, 1024, 2048, 4096, 10000, 20000]\n",
    "\n",
    "# batch_stats: dict[int, dict] — {batch_size: {\"mean_time\": float, \"mean_iterations\": float}}\n",
    "batch_stats = {}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9d8aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e2769fbe13e35e49449e4c1de8e39a",
     "grade": true,
     "grade_id": "cell-9cbb09e22a3b509d",
     "llm_graded": false,
     "locked": true,
     "points": 50,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(batch_stats, dict)\n",
    "assert len(batch_stats) == len(batch_sizes)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    assert bs in batch_stats, f\"Нет записи для batch_size={bs}\"\n",
    "    entry = batch_stats[bs]\n",
    "    assert \"mean_time\" in entry, f'Нет ключа \"mean_time\" для batch_size={bs}'\n",
    "    assert \"mean_iterations\" in entry, f'Нет ключа \"mean_iterations\" для batch_size={bs}'\n",
    "    assert entry[\"mean_time\"] > 0, f\"mean_time должен быть > 0 для batch_size={bs}\"\n",
    "    assert entry[\"mean_iterations\"] > 0, f\"mean_iterations должен быть > 0 для batch_size={bs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb27522",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d10b8703697a647e18e92bd6e95bbd2",
     "grade": false,
     "grade_id": "cell-a3727ef69d87bc3b",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 6.3. График: число итераций до сходимости vs размер батча (0 баллов, необходимо для выводов)\n",
    "\n",
    "Постройте график зависимости среднего числа итераций до сходимости от размера батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf0b7f",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb46530b5e2bff981f5fa647c92a6846",
     "grade": false,
     "grade_id": "cell-f6f45cda999fee4e",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e98e8d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f51b04c17abe8379ed3a07191fddea35",
     "grade": false,
     "grade_id": "cell-c17dd6451c570934",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 6.4. График: время до сходимости vs размер батча (0 баллов, необходимо для выводов)\n",
    "\n",
    "Постройте график зависимости среднего времени до сходимости от размера батча.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80b282",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57560fc241ea0f1056251bac5b70524b",
     "grade": false,
     "grade_id": "cell-5c8cafb1a43369ad",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21edab1",
   "metadata": {},
   "source": [
    "## Выводы о SGD (1 балл)\n",
    "Напишите **в следующей ячейке** свои выводы о влиянии размера батча на работу SGD:\n",
    "- Как размер батча влияет на **число итераций** до сходимости?\n",
    "- Как размер батча влияет на **время** до сходимости? \n",
    "- Какой размер батча вы бы рекомендовали и почему? Существует ли «оптимальный» компромисс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d61465",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "205954229f209230090fdc035f0d2254",
     "grade": true,
     "grade_id": "cell-c73874efa05be99d",
     "llm_graded": true,
     "locked": false,
     "points": 100,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe313c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cafe313c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24125dbd12a8c5fbb17205fe59864f6d",
     "grade": false,
     "grade_id": "cell-107e1ae81b519f58",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Задание 7. Регуляризация (1 балл)\n",
    "\n",
    "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. Напомним, регуляризация – это добавка к функции потерь, которая штрафует за норму весов. Мы будем использовать $L_2$-регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n",
    "$$\n",
    "\n",
    "Вам необходимо вернуться выше в класc `LinearRegression` и добавить  `compute_loss` и `compute_gradients` необходимую часть с коэффицинтом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847c981",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfa4c9a5843de43ce138fe992ca7f148",
     "grade": false,
     "grade_id": "cell-06cc8fa6bdbdbb8b",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 7.1. Функция обучения с регуляризацией (0 баллов, вспомогательная)\n",
    "\n",
    "Реализуйте функцию `train_model_reg`, которая обучает `LinearRegression` с заданным оптимизатором и коэффициентом регуляризации `l2_coef` и возвращает loss на валидационной выборке.\n",
    "\n",
    "Это расширение функции `train_model` из задания 5 с добавлением параметра `l2_coef`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafe132",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6d8a6b3e08c7580e51c46dc7d705053",
     "grade": false,
     "grade_id": "cell-0822063e1b9b5579",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_reg(\n",
    "    x_train, y_train, x_valid, y_valid,\n",
    "    optimizer: BaseDescent,\n",
    "    l2_coef: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Обучает LinearRegression с заданным optimizer и l2_coef.\n",
    "    Возвращает loss на валидационной выборке.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc6249",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d86eee2a13a761140a3d143a7b9ee33d",
     "grade": true,
     "grade_id": "cell-1cedcae1cb709f8a",
     "llm_graded": false,
     "locked": true,
     "points": 0,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_n, _d = 200, 5\n",
    "np.random.seed(42)\n",
    "_X7 = np.random.randn(_n, _d)\n",
    "_y7 = _X7 @ np.random.randn(_d) + np.random.randn(_n) * 0.1\n",
    "_X7_val = np.random.randn(50, _d)\n",
    "_y7_val = _X7_val @ np.random.randn(_d)\n",
    "\n",
    "_opt = VanillaGradientDescent(lr_schedule=TimeDecayLR(lambda_=0.01))\n",
    "_loss_no_reg = train_model_reg(_X7, _y7, _X7_val, _y7_val, _opt, l2_coef=0.0)\n",
    "assert isinstance(_loss_no_reg, (float, np.floating)), \"Функция должна возвращать float\"\n",
    "assert not np.isnan(_loss_no_reg), \"Loss не должен быть NaN\"\n",
    "\n",
    "_opt2 = VanillaGradientDescent(lr_schedule=TimeDecayLR(lambda_=0.01))\n",
    "_loss_with_reg = train_model_reg(_X7, _y7, _X7_val, _y7_val, _opt2, l2_coef=1.0)\n",
    "assert isinstance(_loss_with_reg, (float, np.floating)), \"Функция должна возвращать float\"\n",
    "assert not np.isnan(_loss_with_reg), \"Loss не должен быть NaN\"\n",
    "\n",
    "assert _loss_no_reg != _loss_with_reg, \\\n",
    "    \"Loss с l2_coef=0.0 и l2_coef=1.0 должны отличаться\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a06ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "671080da5b9390db984456759d2a1890",
     "grade": false,
     "grade_id": "cell-1cedcae7cb709f8a",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 7.2. Подбор лучших $\\lambda$ и $\\mu$ с регуляризацией (0.5 балла)\n",
    "\n",
    "Для каждого из четырёх методов подберите по сетке лучшую пару гиперпараметров:\n",
    "- `lambda_` — начальный шаг в `TimeDecayLR`\n",
    "- `mu` — коэффициент $L_2$-регуляризации (`l2_coef` в `LinearRegression`)\n",
    "\n",
    "Сохраните результаты в словарь `report_reg` той же структуры, что и `report` из задания 5, но с дополнительным ключом `\"best_mu\"`:\n",
    "\n",
    "```python\n",
    "report_reg[optimizer_name] = {\n",
    "    \"best_lambda\": ...,\n",
    "    \"best_mu\": ...,\n",
    "    \"r2_test\": ...,\n",
    "    \"r2_train\": ...,\n",
    "    \"loss_history\": ...,        # loss_history на train\n",
    "    \"loss_test\": ...,           # loss на тестовой выборке\n",
    "    \"loss_train\": ...,\n",
    "}\n",
    "```\n",
    "\n",
    "Для финального обучения с лучшими параметрами используйте полную обучающую выборку `x_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f6e73",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8239a3ff18e5347ad08f48770b956ffe",
     "grade": false,
     "grade_id": "cell-dfeed95a68f7a4a7",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizers_reg = {\n",
    "    \"VanillaGradientDescent\": VanillaGradientDescent,\n",
    "    \"StochasticGradientDescent\": StochasticGradientDescent,\n",
    "    \"MomentumDescent\": MomentumDescent,\n",
    "    \"Adam\": Adam,\n",
    "}\n",
    "\n",
    "report_reg = {name: {\"best_lambda\": None, \"best_mu\": None, \"r2_test\": None, \"r2_train\": None, \"loss_history\": None, \"loss_test\": None, \"loss_train\": None}\n",
    "              for name in optimizers_reg}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5b5d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48ecd57c0c0b95f12f45cd6f1ff1420e",
     "grade": true,
     "grade_id": "cell-98eab327cf3767e4",
     "llm_graded": false,
     "locked": true,
     "points": 50,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(report_reg, dict), \"report_reg должен быть словарём\"\n",
    "\n",
    "_required_keys = {\"best_lambda\", \"best_mu\", \"r2_test\", \"r2_train\", \"loss_history\", \"loss_test\", \"loss_train\"}\n",
    "for opt_name in optimizers_reg:\n",
    "    assert opt_name in report_reg, f\"Нет записи для {opt_name}\"\n",
    "    entry = report_reg[opt_name]\n",
    "    for key in _required_keys:\n",
    "        assert key in entry, f'Нет ключа \"{key}\" для {opt_name}'\n",
    "    assert entry[\"r2_test\"] is not None, f\"r2_test не должен быть None для {opt_name}\"\n",
    "    assert entry[\"r2_test\"] > 0, f\"r2_test должен быть > 0 для {opt_name}\"\n",
    "    assert entry[\"best_lambda\"] is not None, f\"best_lambda не должен быть None для {opt_name}\"\n",
    "    assert entry[\"best_mu\"] is not None, f\"best_mu не должен быть None для {opt_name}\"\n",
    "    assert entry[\"best_mu\"] > 0, f\"best_mu должен быть > 0 для {opt_name}\"\n",
    "    assert entry[\"loss_history\"] is not None and len(entry[\"loss_history\"]) > 0, \\\n",
    "        f\"loss_history не должен быть пустым для {opt_name}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64b151",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83907d6af307faf855fb77a97c451804",
     "grade": false,
     "grade_id": "cell-870da0338e5bcdd3",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Задание 7.3. Сравнение: регуляризация vs без регуляризации (0.5 балла за выводы)\n",
    "\n",
    "Постройте для каждого метода **один график** с двумя кривыми loss на обучающей выборке:\n",
    "- Без регуляризации (из `report` задания 5)\n",
    "- С регуляризацией (из `report_reg`)\n",
    "\n",
    "Всего должно получиться **4 графика** (по одному на метод). Не забудьте легенды и заголовки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bcabb",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fe4cc329cb107947d7d77b2dd8de4fc",
     "grade": false,
     "grade_id": "cell-76e26ffd72d363e1",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce7185",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c21ac86198c0e0c3448773a6d33cf9f",
     "grade": false,
     "grade_id": "cell-4c671496638ce53a",
     "llm_graded": false,
     "locked": true,
     "schema_version": 4,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Сравнительная таблица: R² и loss на тесте\n",
    "\n",
    "Выведите таблицу, сравнивающую результаты каждого метода с регуляризацией и без. Включите в таблицу R2 на трейне и тесте, лосс на трейне и тесте.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74538619",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91095a4df287da20983659adc12410b9",
     "grade": false,
     "grade_id": "cell-908f23bc0fa547fa",
     "llm_graded": false,
     "locked": false,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410439c",
   "metadata": {},
   "source": [
    "## Выводы о регуляризации\n",
    "\n",
    "Напишите **в следующей ячейке** свои выводы о влиянии регуляризации:\n",
    "- Как регуляризация повлияла на **сходимость** (скорость, стабильность)?\n",
    "- Как изменилось качество ($R^2$) на **обучающей** выборке?\n",
    "- Как изменилось качество ($R^2$) на **тестовой** выборке? Чем вы можете это объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f603de",
   "metadata": {
    "deletable": false,
    "id": "",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8382dc80911ff8824d32d365c39883a6",
     "grade": true,
     "grade_id": "cell-908f63bc0fa547fa",
     "llm_graded": true,
     "locked": false,
     "points": 50,
     "schema_version": 4,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b3c72",
   "metadata": {},
   "source": [
    "### Фидбек\n",
    "\n",
    "Можете оставить тут мем-ассоциацию с этим дз"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "901a343f",
    "2e877ce0",
    "07913925",
    "cafe313c"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
